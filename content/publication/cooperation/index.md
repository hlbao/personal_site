---
abstract: >-
  Regarding the evolution of cooperation, more generally, agent-based social
  simulation, I have published several papers in decent computer science
  conferences and interdisciplinary journals. I studied the evolution of
  altruism on social networks through reinforcement learning, group
  decision-making, social learning, and reputation-based behavioral regulation:


  -- with social roles and unequal payoff sharing to study the origins of inequality;


  -- with the open-endedness feature of society;


  -- with psychological labs to investigate real-world human behaviors.


  A sample paper is shown below:
slides: ""
url_pdf: ""
publication_types:
  - "2"
  - "1"
  - "3"
authors:
  - admin
author_notes: []
publication: ""
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: Cooperation
doi: https://doi.org/10.1016/j.physa.2017.09.046
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: https://www.dropbox.com/s/0prnzhxgc6zlpri/feature.jpg?dl=0
date: 2021-04-01T19:02:46.486Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: ""
---
This is a long-standing question in game theory: How can we balance social welfare and individual profit? Think about a prisoner's dilemma: The Nash equilibrium (NOT Pareto optimal) is constructed through players' rational choices. So, how can we facilitate altruism and cooperation among selfish agents? More generally, this question is related to coordination, control, contagion, resource allocation, and many other perspectives. All these perspectives link to a kind of ultimate question in classic sociology: the linkage between microscopic behaviors (individual choice) and macroscopic emergence (collective action).

We study this question in a multi-agent simulation environment under different game-theoretical settings, e.g., the coordination game (equilibrium selection or so-called "coordination") and the prisoner's dilemma (cooperation). We apply two forms of interactions: One is traditional game-theoretical interactions, and the other is learning in the game. That is, agents "learn" good behaviors through interaction with others and the feedback from environments. This cool technique is called "reinforcement learning." It is a very vital technique in AI and has successfully been applied in ***[AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far)***. 

Many mechanisms can be used to study cooperation: Agents can vote (collective decision-making); agents can perform reputation-based regulation; new agents can join, and stubborn agents can be removed due to "social pressure" (an open-endedness feature of society); agents can also perform social learning -- learning from culture, imitation through observation...... We can introduce heterogeneity of agents to make the model more realistic. We can also conduct virtual labs to collect experimental data to validate our theories -- it is beneficial and widely studied in social science.

OK, there are so many mechanisms and numerous papers. Nevertheless, what mechanism is the most effective? How to reduce computation complexity and resources of model implementation? (In theory, it is called the "curse of dimensionality." In practice, think about training 10000 agents...) Can these mechanisms be applied in all situations or not? What is the advantage/disadvantage/scope of different mechanisms?

Regarding this topic, the evolution of cooperation, or more generally, agent-based social simulation, I have undertaken profound studies and have ***[published several papers](https://scholar.google.com/citations?user=pSdfiCYAAAAJ&hl=en)*** in decent computer science conferences and interdisciplinary journals in the past few years since my college. (computer scientists prefer to publish their work in conference proceedings. This unusual norm is different from other subjects.)